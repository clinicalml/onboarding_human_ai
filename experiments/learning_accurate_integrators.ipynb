{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab73d94-2803-4c44-b9a9-3820e9840776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../src\")\n",
    "import logging\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "import datetime\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets_hai.bdd import *\n",
    "from datasets_hai.coco import *\n",
    "from datasets_hai.dynasent import *\n",
    "from datasets_hai.gaussians import *\n",
    "from datasets_hai.mmlu import *\n",
    "\n",
    "# local imports\n",
    "from teacher_methods.teacher_domino import *\n",
    "from teacher_methods.teacher_gen import *\n",
    "from teacher_methods.teacher_kmeans import *\n",
    "from teacher_methods.teacher_selection import *\n",
    "from utils.metrics_hai import *\n",
    "from utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a120d9fc-9813-4068-b0c1-618a6c4564ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.0\n",
    "beta_high = 0.5\n",
    "beta_low = 0.00\n",
    "delta = 2\n",
    "randomized_sampling = 1\n",
    "parallel_processes = 1\n",
    "kernel = rbf_kernel # K(.,.)\n",
    "kernel = rbf_kernel # K(.,.)\n",
    "metric_y = loss_01\n",
    "initialization_epochs = 200\n",
    "initialization_restarts = 20\n",
    "TOTAL_TRIALS = 5\n",
    "DATA_SIZES = [5,7,10,12,15,17,20]#,7,9,11,15,17,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a5a859-f375-4496-81d2-e22e1ad390e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# BDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c46422b-bee3-426e-999a-97859049411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.datetime.now()\n",
    "date_now = date_now.strftime(\"%Y-%m-%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1490464-ebb7-4acd-a4d7-814334e09ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pickle.load(open(\"../data/cleaned_pkl/bdd_dataset.pkl\",\"rb\"))\n",
    "data_blurry = pickle.load(open('../data/cleaned_pkl/data_blur_bdd.pkl','rb'))\n",
    "dataset.ai_preds = data_blurry['preds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc72a32-ce5e-477a-97f3-67eec41b5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into 10% train and 90% test, get ids\n",
    "data_size_n = len(dataset.data_y)\n",
    "# PARAMETERS FOR ALGORITHMS\n",
    "epochs = 2000\n",
    "lr = 0.001\n",
    "\n",
    "dataset.metric_y = loss_01\n",
    "\n",
    "\n",
    "metrics_gen_train = []\n",
    "metrics_domino_train = []\n",
    "metrics_kmeans_train = []\n",
    "metrics_selec_train = []\n",
    "\n",
    "metrics_gen_test = []\n",
    "metrics_domino_test = []\n",
    "metrics_kmeans_test = []\n",
    "metrics_selec_test = []\n",
    "\n",
    "metrics_human_alone = []\n",
    "metrics_ai_alone = []\n",
    "\n",
    "\n",
    "max_teaching_points = max(DATA_SIZES)\n",
    "\n",
    "for trial in range(TOTAL_TRIALS):\n",
    "    data_ids = list(range(0, len(dataset.data_y)))\n",
    "    data_train_ids, data_test_ids = train_test_split(data_ids, test_size=0.3)\n",
    "    hum_preds = np.zeros(data_size_n)\n",
    "    # human is right 80% of the time\n",
    "    for i in range(data_size_n):\n",
    "        if dataset.data_y[i] == 1:\n",
    "            hum_preds[i] = np.random.choice([0, 1], p=[0.2, 0.8])\n",
    "        else:\n",
    "            hum_preds[i] = np.random.choice([0, 1], p=[0.8, 0.2])\n",
    "\n",
    "    dataset.hum_preds = hum_preds\n",
    "    prior_rejector_preds = np.array(\n",
    "        [np.random.choice([0, 1], p=[0.5, 0.5]) for i in range(len(dataset.data_y))]\n",
    "    )\n",
    "\n",
    "    teacher_gen = TeacherGenerative(\n",
    "        dataset.data_x[data_train_ids],\n",
    "        dataset.data_y[data_train_ids],\n",
    "        dataset.hum_preds[data_train_ids],\n",
    "        dataset.ai_preds[data_train_ids],\n",
    "        prior_rejector_preds[data_train_ids],\n",
    "        metric_y,\n",
    "        max_teaching_points,\n",
    "        alpha,\n",
    "        beta_high,\n",
    "        beta_low,\n",
    "        delta,\n",
    "    )\n",
    "    teacher_gen.epochs = epochs\n",
    "    teacher_gen.lr = lr\n",
    "    teacher_gen.initialization_restarts = initialization_restarts\n",
    "    teacher_gen.initialization_epochs = initialization_epochs\n",
    "    teacher_gen.fit()\n",
    "\n",
    "    teacher_selection = TeacherSelective(\n",
    "        dataset.data_x[data_train_ids],\n",
    "        dataset.data_y[data_train_ids],\n",
    "        dataset.hum_preds[data_train_ids],\n",
    "        dataset.ai_preds[data_train_ids],\n",
    "        prior_rejector_preds[data_train_ids],\n",
    "        kernel,\n",
    "        metric_y,\n",
    "        max_teaching_points,\n",
    "        alpha,\n",
    "        beta_high,\n",
    "        beta_low,\n",
    "        randomized_sampling,\n",
    "        delta,\n",
    "        parallel_processes,\n",
    "    )\n",
    "    print(\"SELECTION\")\n",
    "    teacher_selection.randomized_sampling = randomized_sampling\n",
    "    teacher_selection.parallel_processes = parallel_processes\n",
    "    teacher_selection.fit()\n",
    "\n",
    "    saved_selec_points = copy.deepcopy(teacher_selection.teaching_set)\n",
    "    saved_gen_points = copy.deepcopy(teacher_gen.teaching_set)\n",
    "    # fit generative and selection\n",
    "    metric_gen_train_trial = []\n",
    "    metric_selec_train_trial = []\n",
    "    metric_gen_test_trial = []\n",
    "    metric_selec_test_trial = []\n",
    "    metric_domino_train_trial = []\n",
    "    metric_domino_test_trial = []\n",
    "    metric_kmeans_train_trial = []\n",
    "    metric_kmeans_test_trial = []\n",
    "    metrics_human_alone_trial = []\n",
    "    metrics_ai_alone_trial = []\n",
    "\n",
    "    for data_size in DATA_SIZES:\n",
    "        try:\n",
    "            teacher_domino = TeacherDomino(\n",
    "                dataset.data_x[data_train_ids],\n",
    "                dataset.data_y[data_train_ids],\n",
    "                dataset.hum_preds[data_train_ids],\n",
    "                dataset.ai_preds[data_train_ids],\n",
    "                dataset.ai_scores[data_train_ids],\n",
    "                metric_y,\n",
    "                n_pca_components=None,\n",
    "                n_mixture_components=data_size,\n",
    "                teaching_points=data_size,\n",
    "            )\n",
    "            teacher_domino.fit()\n",
    "        except:\n",
    "            print(\"Domino failed\")\n",
    "            teacher_domino = TeacherDomino(\n",
    "                dataset.data_x[data_train_ids],\n",
    "                dataset.data_y[data_train_ids],\n",
    "                dataset.hum_preds[data_train_ids],\n",
    "                dataset.ai_preds[data_train_ids],\n",
    "                dataset.ai_scores[data_train_ids],\n",
    "                metric_y,\n",
    "                n_pca_components=None,\n",
    "                n_mixture_components=50,\n",
    "                teaching_points=data_size,\n",
    "            )\n",
    "            teacher_domino.fit()\n",
    "        teacher_kmeans = TeacherKmeans(\n",
    "            dataset.data_x[data_train_ids],\n",
    "            dataset.data_y[data_train_ids],\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            metric_y,\n",
    "            data_size,\n",
    "        )\n",
    "        print(\"Domino\")\n",
    "        teacher_domino.fit()\n",
    "        teacher_kmeans.fit()\n",
    "        teacher_selection.teaching_set = saved_selec_points[:data_size]\n",
    "        teacher_gen.teaching_set = saved_gen_points[:data_size]\n",
    "        # get eval results\n",
    "        # TRAIN FIRST\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(\n",
    "            dataset.data_x[data_train_ids]\n",
    "        )\n",
    "        domino_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            domino_defer_preds,\n",
    "            dataset.data_y[data_train_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(\n",
    "            dataset.data_x[data_train_ids]\n",
    "        )\n",
    "        kmeans_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            kmeans_defer_preds,\n",
    "            dataset.data_y[data_train_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        # trim selection and gen teaching sets\n",
    "\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(\n",
    "            dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids]\n",
    "        )\n",
    "        selection_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            selection_defer_preds,\n",
    "            dataset.data_y[data_train_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(\n",
    "            dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids]\n",
    "        )\n",
    "        gen_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            gen_defer_preds,\n",
    "            dataset.data_y[data_train_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        human_only = compute_metrics(\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            np.zeros(len(data_train_ids)),\n",
    "            dataset.data_y[data_train_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        ai_only = compute_metrics(\n",
    "            dataset.hum_preds[data_train_ids],\n",
    "            dataset.ai_preds[data_train_ids],\n",
    "            np.ones(len(data_train_ids)),\n",
    "            dataset.data_y[data_train_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        metric_gen_train_trial.append(gen_loss)\n",
    "        metric_selec_train_trial.append(selection_loss)\n",
    "        metric_domino_train_trial.append(domino_loss)\n",
    "        metric_kmeans_train_trial.append(kmeans_loss)\n",
    "        metrics_human_alone_trial.append(human_only)\n",
    "        metrics_ai_alone_trial.append(ai_only)\n",
    "        # TEST, same code but use data_test_ids\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(\n",
    "            dataset.data_x[data_test_ids]\n",
    "        )\n",
    "        domino_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_test_ids],\n",
    "            dataset.ai_preds[data_test_ids],\n",
    "            domino_defer_preds,\n",
    "            dataset.data_y[data_test_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(\n",
    "            dataset.data_x[data_test_ids]\n",
    "        )\n",
    "        kmeans_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_test_ids],\n",
    "            dataset.ai_preds[data_test_ids],\n",
    "            kmeans_defer_preds,\n",
    "            dataset.data_y[data_test_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(\n",
    "            dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids]\n",
    "        )\n",
    "        selection_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_test_ids],\n",
    "            dataset.ai_preds[data_test_ids],\n",
    "            selection_defer_preds,\n",
    "            dataset.data_y[data_test_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(\n",
    "            dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids]\n",
    "        )\n",
    "        gen_loss = compute_metrics(\n",
    "            dataset.hum_preds[data_test_ids],\n",
    "            dataset.ai_preds[data_test_ids],\n",
    "            gen_defer_preds,\n",
    "            dataset.data_y[data_test_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        human_only = compute_metrics(\n",
    "            dataset.hum_preds[data_test_ids],\n",
    "            dataset.ai_preds[data_test_ids],\n",
    "            np.zeros(len(data_test_ids)),\n",
    "            dataset.data_y[data_test_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        ai_only = compute_metrics(\n",
    "            dataset.hum_preds[data_test_ids],\n",
    "            dataset.ai_preds[data_test_ids],\n",
    "            np.ones(len(data_test_ids)),\n",
    "            dataset.data_y[data_test_ids],\n",
    "            metric_y,\n",
    "        )\n",
    "        metric_gen_test_trial.append(gen_loss)\n",
    "        metric_selec_test_trial.append(selection_loss)\n",
    "        metric_domino_test_trial.append(domino_loss)\n",
    "        metric_kmeans_test_trial.append(kmeans_loss)\n",
    "\n",
    "    metrics_human_alone.append(metrics_human_alone_trial)\n",
    "    metrics_ai_alone.append(metrics_ai_alone_trial)\n",
    "    metrics_gen_train.append(metric_gen_train_trial)\n",
    "    metrics_selec_train.append(metric_selec_train_trial)\n",
    "    metrics_domino_train.append(metric_domino_train_trial)\n",
    "    metrics_kmeans_train.append(metric_kmeans_train_trial)\n",
    "    metrics_gen_test.append(metric_gen_test_trial)\n",
    "    metrics_selec_test.append(metric_selec_test_trial)\n",
    "    metrics_domino_test.append(metric_domino_test_trial)\n",
    "    metrics_kmeans_test.append(metric_kmeans_test_trial)\n",
    "    data_save = {}\n",
    "    data_save[\"metrics_human_alone\"] = metrics_human_alone\n",
    "    data_save[\"metrics_ai_alone\"] = metrics_ai_alone\n",
    "    data_save[\"metrics_gen_train\"] = metrics_gen_train\n",
    "    data_save[\"metrics_selec_train\"] = metrics_selec_train\n",
    "    data_save[\"metrics_domino_train\"] = metrics_domino_train\n",
    "    data_save[\"metrics_kmeans_train\"] = metrics_kmeans_train\n",
    "    data_save[\"metrics_gen_test\"] = metrics_gen_test\n",
    "    data_save[\"metrics_selec_test\"] = metrics_selec_test\n",
    "    data_save[\"metrics_domino_test\"] = metrics_domino_test\n",
    "    data_save[\"metrics_kmeans_test\"] = metrics_kmeans_test\n",
    "    data_save[\"date\"] = datetime.datetime.now()\n",
    "    data_save[\"TOTAL_TRIALS\"] = TOTAL_TRIALS\n",
    "    data_save[\"DATA_SIZES\"] = DATA_SIZES\n",
    "    with open(\"../exp_data/results/bdd_real_\" + date_now + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_save, f)\n",
    "    print(\"trial\", trial, \"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130418b6-c140-4134-88fc-86500314ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"../\")\n",
    "#matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "#matplotlib.rcParams['ps.fonttype'] = 42\n",
    "#plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Train Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "\n",
    "#plt.savefig(\"../exp_data/plots/plot_synth_data_realizable_\"+ date_now+\".pdf\", dpi = 1000, bbox_inches='tight')\n",
    "#plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700042ec-189b-4ea3-80b4-f8dcfe175f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Test Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace5023-2906-4861-b5f3-3c528f17ab7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ada44-d741-4072-bca4-31bbef01bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.datetime.now()\n",
    "date_now = date_now.strftime(\"%Y-%m-%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878cd0d1-ae67-4a03-a5c8-1cc2f9101b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  pickle.load(open('../data/cleaned_pkl/mmlu_dataset.pkl',\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a85a5d9-7c48-4bce-8b1b-b5d594ebbd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split data into 10% train and 90% test, get ids\n",
    "data_size_n = len(dataset.data_y)\n",
    "# PARAMETERS FOR ALGORITHMS\n",
    "lr = 0.001\n",
    "epochs = 2000\n",
    "metrics_gen_train = []\n",
    "metrics_domino_train = []\n",
    "metrics_kmeans_train = []\n",
    "metrics_selec_train = []\n",
    "\n",
    "metrics_gen_test = []\n",
    "metrics_domino_test = []\n",
    "metrics_kmeans_test = []\n",
    "metrics_selec_test = []\n",
    "\n",
    "metrics_human_alone = []\n",
    "metrics_ai_alone = []\n",
    "\n",
    "\n",
    "max_teaching_points = max(DATA_SIZES)\n",
    "\n",
    "for trial in range(TOTAL_TRIALS):\n",
    "    data_ids = list(range(0,len(dataset.data_y) ))\n",
    "    data_train_ids, data_test_ids = train_test_split(data_ids, test_size=0.3)\n",
    "    hum_preds = np.zeros(data_size_n)\n",
    "    # human is right 80% of the time\n",
    "    hum_preds = np.empty(data_size_n, dtype=int)\n",
    "\n",
    "    for i in range(len(dataset.data_y)):\n",
    "        current_value = dataset.data_y[i]\n",
    "        # 50% chance to match the current_value\n",
    "        if np.random.rand() < 0.5:\n",
    "            hum_preds[i] = current_value\n",
    "        else:\n",
    "            # Choose a random value excluding the current_value\n",
    "            possible_values = [v for v in [0, 1, 2, 3] if v != current_value]\n",
    "            hum_preds[i] = np.random.choice(possible_values)\n",
    "            \n",
    "    dataset.hum_preds = hum_preds\n",
    "    prior_rejector_preds = np.array([np.random.choice([0,1], p=[0.5,0.5]) for i in range(len(dataset.data_y))])\n",
    "    \n",
    "    teacher_gen = TeacherGenerative(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids],\n",
    "                                    prior_rejector_preds[data_train_ids], metric_y, max_teaching_points, alpha, beta_high, beta_low, delta)\n",
    "    teacher_gen.epochs = epochs\n",
    "    teacher_gen.lr = lr\n",
    "    teacher_gen.initialization_restarts = initialization_restarts\n",
    "    teacher_gen.initialization_epochs = initialization_epochs\n",
    "    teacher_gen.fit()\n",
    "\n",
    "    teacher_selection = TeacherSelective(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids],\n",
    "                                         prior_rejector_preds[data_train_ids], kernel, metric_y, max_teaching_points, alpha, beta_high, beta_low, randomized_sampling, delta, parallel_processes)\n",
    "    print(\"SELECTION\")\n",
    "    teacher_selection.randomized_sampling = randomized_sampling\n",
    "    teacher_selection.parallel_processes = parallel_processes\n",
    "    teacher_selection.fit()\n",
    "\n",
    "    saved_selec_points = copy.deepcopy(teacher_selection.teaching_set)\n",
    "    saved_gen_points = copy.deepcopy(teacher_gen.teaching_set)\n",
    "    # fit generative and selection\n",
    "    metric_gen_train_trial = []\n",
    "    metric_selec_train_trial = []\n",
    "    metric_gen_test_trial = []\n",
    "    metric_selec_test_trial = []\n",
    "    metric_domino_train_trial = []\n",
    "    metric_domino_test_trial = []\n",
    "    metric_kmeans_train_trial = []\n",
    "    metric_kmeans_test_trial = []\n",
    "    metrics_human_alone_trial = []\n",
    "    metrics_ai_alone_trial = []\n",
    "\n",
    "    for data_size in DATA_SIZES:\n",
    "        data_y_one_hot = np.zeros((len(data_train_ids), 4))\n",
    "        data_y_one_hot[np.arange(len(data_train_ids)), dataset.data_y[data_train_ids]] = 1\n",
    "        ai_preds_one_hot = np.zeros((len(data_train_ids), 4))\n",
    "        ai_preds_one_hot[np.arange(len(data_train_ids)), dataset.ai_preds[data_train_ids]] = 1\n",
    "        hum_preds_one_hot = np.zeros((len(data_train_ids), 4))\n",
    "        dataset.hum_preds = dataset.hum_preds.astype(int)\n",
    "        hum_preds_one_hot[np.arange(len(data_train_ids)), dataset.hum_preds[data_train_ids]] = 1\n",
    "        try:\n",
    "            teacher_domino = TeacherDomino(dataset.data_x[data_train_ids], data_y_one_hot,hum_preds_one_hot,ai_preds_one_hot, ai_preds_one_hot, metric_y, n_pca_components = None, n_mixture_components = data_size , teaching_points = data_size)\n",
    "            teacher_domino.fit()\n",
    "        except:\n",
    "            print(\"Domino failed\")\n",
    "            teacher_domino = TeacherDomino(dataset.data_x[data_train_ids], data_y_one_hot,hum_preds_one_hot,ai_preds_one_hot, ai_preds_one_hot, metric_y, n_pca_components = None, n_mixture_components = 50 , teaching_points = data_size)\n",
    "            teacher_domino.fit()          \n",
    "        teacher_kmeans = TeacherKmeans(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], metric_y, data_size)\n",
    "        print(\"Domino\")\n",
    "        teacher_domino.fit()\n",
    "        teacher_kmeans.fit()\n",
    "        teacher_selection.teaching_set = saved_selec_points[:data_size]\n",
    "        teacher_gen.teaching_set = saved_gen_points[:data_size]\n",
    "        # get eval results\n",
    "        # TRAIN FIRST\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(dataset.data_x[data_train_ids])\n",
    "        domino_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], domino_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(dataset.data_x[data_train_ids])\n",
    "        kmeans_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], kmeans_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        # trim selection and gen teaching sets\n",
    "\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids])\n",
    "        selection_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], selection_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids])\n",
    "        gen_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], gen_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        human_only = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], np.zeros(len(data_train_ids)), dataset.data_y[data_train_ids], metric_y)\n",
    "        ai_only = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], np.ones(len(data_train_ids)), dataset.data_y[data_train_ids], metric_y)\n",
    "        metric_gen_train_trial.append(gen_loss)\n",
    "        metric_selec_train_trial.append(selection_loss)\n",
    "        metric_domino_train_trial.append(domino_loss)\n",
    "        metric_kmeans_train_trial.append(kmeans_loss)\n",
    "        metrics_human_alone_trial.append(human_only)\n",
    "        metrics_ai_alone_trial.append(ai_only)\n",
    "        # TEST, same code but use data_test_ids\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(dataset.data_x[data_test_ids])\n",
    "        domino_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], domino_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(dataset.data_x[data_test_ids])\n",
    "        kmeans_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], kmeans_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids])\n",
    "        selection_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], selection_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids])\n",
    "        gen_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], gen_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        human_only = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], np.zeros(len(data_test_ids)), dataset.data_y[data_test_ids], metric_y)\n",
    "        ai_only = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], np.ones(len(data_test_ids)), dataset.data_y[data_test_ids], metric_y)\n",
    "        metric_gen_test_trial.append(gen_loss)\n",
    "        metric_selec_test_trial.append(selection_loss)\n",
    "        metric_domino_test_trial.append(domino_loss)\n",
    "        metric_kmeans_test_trial.append(kmeans_loss)\n",
    "        \n",
    "    metrics_human_alone.append(metrics_human_alone_trial)\n",
    "    metrics_ai_alone.append(metrics_ai_alone_trial)\n",
    "    metrics_gen_train.append(metric_gen_train_trial)\n",
    "    metrics_selec_train.append(metric_selec_train_trial)\n",
    "    metrics_domino_train.append(metric_domino_train_trial)\n",
    "    metrics_kmeans_train.append(metric_kmeans_train_trial)\n",
    "    metrics_gen_test.append(metric_gen_test_trial)\n",
    "    metrics_selec_test.append(metric_selec_test_trial)\n",
    "    metrics_domino_test.append(metric_domino_test_trial)\n",
    "    metrics_kmeans_test.append(metric_kmeans_test_trial)\n",
    "    data_save = {}\n",
    "    data_save[\"metrics_human_alone\"] = metrics_human_alone\n",
    "    data_save[\"metrics_ai_alone\"] = metrics_ai_alone\n",
    "    data_save[\"metrics_gen_train\"] = metrics_gen_train\n",
    "    data_save[\"metrics_selec_train\"] = metrics_selec_train\n",
    "    data_save[\"metrics_domino_train\"] = metrics_domino_train\n",
    "    data_save[\"metrics_kmeans_train\"] = metrics_kmeans_train\n",
    "    data_save[\"metrics_gen_test\"] = metrics_gen_test\n",
    "    data_save[\"metrics_selec_test\"] = metrics_selec_test\n",
    "    data_save[\"metrics_domino_test\"] = metrics_domino_test\n",
    "    data_save[\"metrics_kmeans_test\"] = metrics_kmeans_test\n",
    "    data_save['date'] = datetime.datetime.now()\n",
    "    data_save['TOTAL_TRIALS'] = TOTAL_TRIALS\n",
    "    data_save['DATA_SIZES'] = DATA_SIZES\n",
    "    with open(\"../exp_data/results/mmlu_real_\" + date_now + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_save, f)\n",
    "    print(\"trial\", trial, \"done\")\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2f688c-17d4-42a2-9a70-8ea4652dc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"../\")\n",
    "#matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "#matplotlib.rcParams['ps.fonttype'] = 42\n",
    "#plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Train Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "\n",
    "#plt.savefig(\"../exp_data/plots/plot_synth_data_realizable_\"+ date_now+\".pdf\", dpi = 1000, bbox_inches='tight')\n",
    "#plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c59e5d8-db0c-4bad-a626-1a8a37ad17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Test Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "\n",
    "#plt.savefig(\"../exp_data/plots/plot_synth_data_realizable_\"+ date_now+\".pdf\", dpi = 1000, bbox_inches='tight')\n",
    "#plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112e886-ecd4-4322-819f-dcfafeff7196",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MS-COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2544bb-25aa-4a23-a3c4-058cc2856f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.datetime.now()\n",
    "date_now = date_now.strftime(\"%Y-%m-%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6684fc7-1e22-4b5d-8a3f-8fefd6610bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  pickle.load(open('../data/cleaned_pkl/coco_dataset.pkl',\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2536054b-c095-4b88-98f8-b8c337c094ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_n = len(dataset.data_y)\n",
    "# PARAMETERS FOR ALGORITHMS\n",
    "lr = 0.001\n",
    "epochs = 2000\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics_gen_train = []\n",
    "metrics_domino_train = []\n",
    "metrics_kmeans_train = []\n",
    "metrics_selec_train = []\n",
    "\n",
    "metrics_gen_test = []\n",
    "metrics_domino_test = []\n",
    "metrics_kmeans_test = []\n",
    "metrics_selec_test = []\n",
    "\n",
    "metrics_human_alone = []\n",
    "metrics_ai_alone = []\n",
    "\n",
    "\n",
    "max_teaching_points = max(DATA_SIZES)\n",
    "\n",
    "for trial in range(TOTAL_TRIALS):\n",
    "    data_ids = list(range(0,len(dataset.data_y) ))\n",
    "    data_train_ids, data_test_ids = train_test_split(data_ids, test_size=0.3)\n",
    "    hum_preds = np.zeros(data_size_n)\n",
    "    # human is right 80% of the time\n",
    "    for i in range(data_size_n):\n",
    "        if dataset.data_y[i] == 1:\n",
    "            hum_preds[i] = np.random.choice([0,1], p=[0.3, 0.7])\n",
    "        else:\n",
    "            hum_preds[i] = np.random.choice([0,1], p=[0.7, 0.3])\n",
    "            \n",
    "    dataset.hum_preds = hum_preds\n",
    "    prior_rejector_preds = np.array([np.random.choice([0,1], p=[0.5,0.5]) for i in range(len(dataset.data_y))])\n",
    "\n",
    "\n",
    "\n",
    "    teacher_gen = TeacherGenerative(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids],\n",
    "                                    prior_rejector_preds[data_train_ids], metric_y, max_teaching_points, alpha, beta_high, beta_low, delta)\n",
    "    teacher_gen.epochs = epochs\n",
    "    teacher_gen.lr = lr\n",
    "    teacher_gen.initialization_restarts = initialization_restarts\n",
    "    teacher_gen.initialization_epochs = initialization_epochs\n",
    "    teacher_gen.fit()\n",
    "\n",
    "    teacher_selection = TeacherSelective(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids],\n",
    "                                         prior_rejector_preds[data_train_ids], kernel, metric_y, max_teaching_points, alpha, beta_high, beta_low, randomized_sampling, delta, parallel_processes)\n",
    "    print(\"SELECTION\")\n",
    "    teacher_selection.randomized_sampling = randomized_sampling\n",
    "    teacher_selection.parallel_processes = parallel_processes\n",
    "    teacher_selection.fit()\n",
    "\n",
    "    saved_selec_points = copy.deepcopy(teacher_selection.teaching_set)\n",
    "    saved_gen_points = copy.deepcopy(teacher_gen.teaching_set)\n",
    "    # fit generative and selection\n",
    "    metric_gen_train_trial = []\n",
    "    metric_selec_train_trial = []\n",
    "    metric_gen_test_trial = []\n",
    "    metric_selec_test_trial = []\n",
    "    metric_domino_train_trial = []\n",
    "    metric_domino_test_trial = []\n",
    "    metric_kmeans_train_trial = []\n",
    "    metric_kmeans_test_trial = []\n",
    "    metrics_human_alone_trial = []\n",
    "    metrics_ai_alone_trial = []\n",
    "\n",
    "    for data_size in DATA_SIZES:\n",
    "        try:\n",
    "            teacher_domino = TeacherDomino(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], dataset.ai_scores[data_train_ids], metric_y, n_pca_components = None, n_mixture_components = data_size , teaching_points = data_size)\n",
    "            teacher_domino.fit()\n",
    "        except:\n",
    "            print(\"Domino failed\")\n",
    "            teacher_domino = TeacherDomino(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], dataset.ai_scores[data_train_ids], metric_y, n_pca_components = None, n_mixture_components = 50 , teaching_points = data_size)\n",
    "            teacher_domino.fit()            \n",
    "        teacher_kmeans = TeacherKmeans(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], metric_y, data_size)\n",
    "        print(\"Domino\")\n",
    "        teacher_domino.fit()\n",
    "        teacher_kmeans.fit()\n",
    "        teacher_selection.teaching_set = saved_selec_points[:data_size]\n",
    "        teacher_gen.teaching_set = saved_gen_points[:data_size]\n",
    "        # get eval results\n",
    "        # TRAIN FIRST\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(dataset.data_x[data_train_ids])\n",
    "        domino_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], domino_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(dataset.data_x[data_train_ids])\n",
    "        kmeans_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], kmeans_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        # trim selection and gen teaching sets\n",
    "\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids])\n",
    "        selection_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], selection_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids])\n",
    "        gen_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], gen_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        human_only = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], np.zeros(len(data_train_ids)), dataset.data_y[data_train_ids], metric_y)\n",
    "        ai_only = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], np.ones(len(data_train_ids)), dataset.data_y[data_train_ids], metric_y)\n",
    "        metric_gen_train_trial.append(gen_loss)\n",
    "        metric_selec_train_trial.append(selection_loss)\n",
    "        metric_domino_train_trial.append(domino_loss)\n",
    "        metric_kmeans_train_trial.append(kmeans_loss)\n",
    "        metrics_human_alone_trial.append(human_only)\n",
    "        metrics_ai_alone_trial.append(ai_only)\n",
    "        # TEST, same code but use data_test_ids\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(dataset.data_x[data_test_ids])\n",
    "        domino_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], domino_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(dataset.data_x[data_test_ids])\n",
    "        kmeans_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], kmeans_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids])\n",
    "        selection_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], selection_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids])\n",
    "        gen_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], gen_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        human_only = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], np.zeros(len(data_test_ids)), dataset.data_y[data_test_ids], metric_y)\n",
    "        ai_only = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], np.ones(len(data_test_ids)), dataset.data_y[data_test_ids], metric_y)\n",
    "        metric_gen_test_trial.append(gen_loss)\n",
    "        metric_selec_test_trial.append(selection_loss)\n",
    "        metric_domino_test_trial.append(domino_loss)\n",
    "        metric_kmeans_test_trial.append(kmeans_loss)\n",
    "        \n",
    "    metrics_human_alone.append(metrics_human_alone_trial)\n",
    "    metrics_ai_alone.append(metrics_ai_alone_trial)\n",
    "    metrics_gen_train.append(metric_gen_train_trial)\n",
    "    metrics_selec_train.append(metric_selec_train_trial)\n",
    "    metrics_domino_train.append(metric_domino_train_trial)\n",
    "    metrics_kmeans_train.append(metric_kmeans_train_trial)\n",
    "    metrics_gen_test.append(metric_gen_test_trial)\n",
    "    metrics_selec_test.append(metric_selec_test_trial)\n",
    "    metrics_domino_test.append(metric_domino_test_trial)\n",
    "    metrics_kmeans_test.append(metric_kmeans_test_trial)\n",
    "    data_save = {}\n",
    "    data_save[\"metrics_human_alone\"] = metrics_human_alone\n",
    "    data_save[\"metrics_ai_alone\"] = metrics_ai_alone\n",
    "    data_save[\"metrics_gen_train\"] = metrics_gen_train\n",
    "    data_save[\"metrics_selec_train\"] = metrics_selec_train\n",
    "    data_save[\"metrics_domino_train\"] = metrics_domino_train\n",
    "    data_save[\"metrics_kmeans_train\"] = metrics_kmeans_train\n",
    "    data_save[\"metrics_gen_test\"] = metrics_gen_test\n",
    "    data_save[\"metrics_selec_test\"] = metrics_selec_test\n",
    "    data_save[\"metrics_domino_test\"] = metrics_domino_test\n",
    "    data_save[\"metrics_kmeans_test\"] = metrics_kmeans_test\n",
    "    data_save['date'] = datetime.datetime.now()\n",
    "    data_save['TOTAL_TRIALS'] = TOTAL_TRIALS\n",
    "    data_save['DATA_SIZES'] = DATA_SIZES\n",
    "    with open(\"../exp_data/results/coco_real_\" + date_now + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_save, f)\n",
    "    print(\"trial\", trial, \"done\")\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02712c4b-6299-446e-99bd-8bde1b66ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"../\")\n",
    "#matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "#matplotlib.rcParams['ps.fonttype'] = 42\n",
    "#plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Train Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3e3a5-c6a6-4ac7-be15-4a03ca516f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Test Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n",
    "\n",
    "#plt.savefig(\"../exp_data/plots/plot_synth_data_realizable_\"+ date_now+\".pdf\", dpi = 1000, bbox_inches='tight')\n",
    "#plt.show()\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae287504-d4ad-4344-abdc-4a23978798ee",
   "metadata": {
    "tags": []
   },
   "source": [
    "# DYNASENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e9040-19eb-4ccf-93dd-becf1ec26dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_now = datetime.datetime.now()\n",
    "date_now = date_now.strftime(\"%Y-%m-%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea7e0fb-bd09-4fb0-8ee8-9f79fc77bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset =  pickle.load(open('../data/cleaned_pkl/dynasent_dataset.pkl',\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb6154-48e6-41bd-8fdd-f67ad41b6016",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size_n = len(dataset.data_y)\n",
    "# PARAMETERS FOR ALGORITHMS\n",
    "lr = 0.001\n",
    "epochs = 2000\n",
    "\n",
    "\n",
    "metrics_gen_train = []\n",
    "metrics_domino_train = []\n",
    "metrics_kmeans_train = []\n",
    "metrics_selec_train = []\n",
    "\n",
    "metrics_gen_test = []\n",
    "metrics_domino_test = []\n",
    "metrics_kmeans_test = []\n",
    "metrics_selec_test = []\n",
    "\n",
    "metrics_human_alone = []\n",
    "metrics_ai_alone = []\n",
    "\n",
    "\n",
    "max_teaching_points = max(DATA_SIZES)\n",
    "\n",
    "for trial in range(TOTAL_TRIALS):\n",
    "    data_ids = list(range(0,len(dataset.data_y) ))\n",
    "    data_train_ids, data_test_ids = train_test_split(data_ids, test_size=0.3)\n",
    "\n",
    "    prior_rejector_preds = np.array([np.random.choice([0,1], p=[1,0]) for i in range(len(dataset.data_y))])\n",
    "\n",
    "\n",
    "\n",
    "    teacher_gen = TeacherGenerative(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids],\n",
    "                                    prior_rejector_preds[data_train_ids], metric_y, max_teaching_points, alpha, beta_high, beta_low, delta)\n",
    "    teacher_gen.epochs = epochs\n",
    "    teacher_gen.lr = lr\n",
    "    teacher_gen.initialization_restarts = initialization_restarts\n",
    "    teacher_gen.initialization_epochs = initialization_epochs\n",
    "    teacher_gen.fit()\n",
    "\n",
    "    teacher_selection = TeacherSelective(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids],\n",
    "                                         prior_rejector_preds[data_train_ids], kernel, metric_y, max_teaching_points, alpha, beta_high, beta_low, randomized_sampling, delta, parallel_processes)\n",
    "    print(\"SELECTION\")\n",
    "    teacher_selection.randomized_sampling = randomized_sampling\n",
    "    teacher_selection.parallel_processes = parallel_processes\n",
    "    teacher_selection.fit()\n",
    "\n",
    "    saved_selec_points = copy.deepcopy(teacher_selection.teaching_set)\n",
    "    saved_gen_points = copy.deepcopy(teacher_gen.teaching_set)\n",
    "    # fit generative and selection\n",
    "    metric_gen_train_trial = []\n",
    "    metric_selec_train_trial = []\n",
    "    metric_gen_test_trial = []\n",
    "    metric_selec_test_trial = []\n",
    "    metric_domino_train_trial = []\n",
    "    metric_domino_test_trial = []\n",
    "    metric_kmeans_train_trial = []\n",
    "    metric_kmeans_test_trial = []\n",
    "    metrics_human_alone_trial = []\n",
    "    metrics_ai_alone_trial = []\n",
    "\n",
    "    for data_size in DATA_SIZES:\n",
    "        data_y_one_hot = np.zeros((len(data_train_ids), 4))\n",
    "        data_y_one_hot[np.arange(len(data_train_ids)), dataset.data_y[data_train_ids]] = 1\n",
    "        ai_preds_one_hot = np.zeros((len(data_train_ids), 4))\n",
    "        ai_preds_one_hot[np.arange(len(data_train_ids)), dataset.ai_preds[data_train_ids]] = 1\n",
    "        hum_preds_one_hot = np.zeros((len(data_train_ids), 4))\n",
    "        dataset.hum_preds = dataset.hum_preds.astype(int)\n",
    "        hum_preds_one_hot[np.arange(len(data_train_ids)), dataset.hum_preds[data_train_ids]] = 1\n",
    "        try:\n",
    "            teacher_domino = TeacherDomino(dataset.data_x[data_train_ids], data_y_one_hot,hum_preds_one_hot,ai_preds_one_hot, ai_preds_one_hot, metric_y, n_pca_components = None, n_mixture_components = data_size , teaching_points = data_size)\n",
    "            teacher_domino.fit()\n",
    "        except:\n",
    "            print(\"Domino failed\")\n",
    "            teacher_domino = TeacherDomino(dataset.data_x[data_train_ids], data_y_one_hot,hum_preds_one_hot,ai_preds_one_hot, ai_preds_one_hot, metric_y, n_pca_components = None, n_mixture_components = 50 , teaching_points = data_size)\n",
    "            teacher_domino.fit()           \n",
    "        teacher_kmeans = TeacherKmeans(dataset.data_x[data_train_ids], dataset.data_y[data_train_ids], dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], metric_y, data_size)\n",
    "        print(\"Domino\")\n",
    "        teacher_domino.fit()\n",
    "        teacher_kmeans.fit()\n",
    "        teacher_selection.teaching_set = saved_selec_points[:data_size]\n",
    "        teacher_gen.teaching_set = saved_gen_points[:data_size]\n",
    "        # get eval results\n",
    "        # TRAIN FIRST\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(dataset.data_x[data_train_ids])\n",
    "        domino_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], domino_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(dataset.data_x[data_train_ids])\n",
    "        kmeans_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], kmeans_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        # trim selection and gen teaching sets\n",
    "\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids])\n",
    "        selection_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], selection_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(dataset.data_x[data_train_ids], prior_rejector_preds[data_train_ids])\n",
    "        gen_loss = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], gen_defer_preds, dataset.data_y[data_train_ids], metric_y)\n",
    "        human_only = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], np.zeros(len(data_train_ids)), dataset.data_y[data_train_ids], metric_y)\n",
    "        ai_only = compute_metrics( dataset.hum_preds[data_train_ids], dataset.ai_preds[data_train_ids], np.ones(len(data_train_ids)), dataset.data_y[data_train_ids], metric_y)\n",
    "        metric_gen_train_trial.append(gen_loss)\n",
    "        metric_selec_train_trial.append(selection_loss)\n",
    "        metric_domino_train_trial.append(domino_loss)\n",
    "        metric_kmeans_train_trial.append(kmeans_loss)\n",
    "        metrics_human_alone_trial.append(human_only)\n",
    "        metrics_ai_alone_trial.append(ai_only)\n",
    "        # TEST, same code but use data_test_ids\n",
    "        domino_defer_preds = teacher_domino.get_defer_preds(dataset.data_x[data_test_ids])\n",
    "        domino_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], domino_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        kmeans_defer_preds = teacher_kmeans.get_defer_preds(dataset.data_x[data_test_ids])\n",
    "        kmeans_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], kmeans_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        selection_defer_preds = teacher_selection.get_defer_preds(dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids])\n",
    "        selection_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], selection_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        gen_defer_preds = teacher_gen.get_defer_preds(dataset.data_x[data_test_ids], prior_rejector_preds[data_test_ids])\n",
    "        gen_loss = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], gen_defer_preds, dataset.data_y[data_test_ids], metric_y)\n",
    "        human_only = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], np.zeros(len(data_test_ids)), dataset.data_y[data_test_ids], metric_y)\n",
    "        ai_only = compute_metrics( dataset.hum_preds[data_test_ids], dataset.ai_preds[data_test_ids], np.ones(len(data_test_ids)), dataset.data_y[data_test_ids], metric_y)\n",
    "        metric_gen_test_trial.append(gen_loss)\n",
    "        metric_selec_test_trial.append(selection_loss)\n",
    "        metric_domino_test_trial.append(domino_loss)\n",
    "        metric_kmeans_test_trial.append(kmeans_loss)\n",
    "        \n",
    "    metrics_human_alone.append(metrics_human_alone_trial)\n",
    "    metrics_ai_alone.append(metrics_ai_alone_trial)\n",
    "    metrics_gen_train.append(metric_gen_train_trial)\n",
    "    metrics_selec_train.append(metric_selec_train_trial)\n",
    "    metrics_domino_train.append(metric_domino_train_trial)\n",
    "    metrics_kmeans_train.append(metric_kmeans_train_trial)\n",
    "    metrics_gen_test.append(metric_gen_test_trial)\n",
    "    metrics_selec_test.append(metric_selec_test_trial)\n",
    "    metrics_domino_test.append(metric_domino_test_trial)\n",
    "    metrics_kmeans_test.append(metric_kmeans_test_trial)\n",
    "    data_save = {}\n",
    "    data_save[\"metrics_human_alone\"] = metrics_human_alone\n",
    "    data_save[\"metrics_ai_alone\"] = metrics_ai_alone\n",
    "    data_save[\"metrics_gen_train\"] = metrics_gen_train\n",
    "    data_save[\"metrics_selec_train\"] = metrics_selec_train\n",
    "    data_save[\"metrics_domino_train\"] = metrics_domino_train\n",
    "    data_save[\"metrics_kmeans_train\"] = metrics_kmeans_train\n",
    "    data_save[\"metrics_gen_test\"] = metrics_gen_test\n",
    "    data_save[\"metrics_selec_test\"] = metrics_selec_test\n",
    "    data_save[\"metrics_domino_test\"] = metrics_domino_test\n",
    "    data_save[\"metrics_kmeans_test\"] = metrics_kmeans_test\n",
    "    data_save['date'] = datetime.datetime.now()\n",
    "    data_save['TOTAL_TRIALS'] = TOTAL_TRIALS\n",
    "    data_save['DATA_SIZES'] = DATA_SIZES\n",
    "    with open(\"../exp_data/results/dynasent_real_\" + date_now + \".pkl\", \"wb\") as f:\n",
    "        pickle.dump(data_save, f)\n",
    "    print(\"trial\", trial, \"done\")\n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362381eb-6527-409f-904e-3aba014ccfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sys.path.append(\"../\")\n",
    "#matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "#matplotlib.rcParams['ps.fonttype'] = 42\n",
    "#plt.rc('text', usetex=True)\n",
    "#plt.rc('font', family='serif')\n",
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_train[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_train[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_train[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Train Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01801c-6901-4a89-b379-7ecfad385b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "alpha_overlap = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# printing from a pickle file\n",
    "max_trials = TOTAL_TRIALS\n",
    "ns = DATA_SIZES\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_gen_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_gen_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"d\",  label=f'GEN (ours)', alpha =alpha_overlap, color = \"black\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_domino_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_domino_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"o\",  label=f'DOMINO', alpha =alpha_overlap, color = \"red\")\n",
    "\n",
    "avgs_rand = [np.average([metrics_kmeans_test[triall][i][1]['score']   for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_kmeans_test[triall][i][1]['score']  for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"x\",  label=f'KMEANS', alpha =alpha_overlap, color = \"blue\")\n",
    "\n",
    "\n",
    "avgs_rand = [np.average([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "stds_rand = [np.std([metrics_selec_test[triall][i][1]['score']     for triall in range(max_trials)]) for i in range(len(ns))]\n",
    "plt.errorbar(ns,  avgs_rand, yerr=stds_rand, marker = \"<\",  label=f'SELECT', alpha =alpha_overlap, color = \"green\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.get_xaxis().tick_bottom()    \n",
    "ax.get_yaxis().tick_left()   \n",
    "plt.grid()\n",
    "plt.legend(fontsize='xx-large')\n",
    "plt.legend()\n",
    "plt.ylabel('Test Loss ', fontsize='xx-large')\n",
    "plt.xlabel('Training data size', fontsize='xx-large')\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 6\n",
    "fig_size[1] = 4.2\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:teach_vision] *",
   "language": "python",
   "name": "conda-env-teach_vision-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
